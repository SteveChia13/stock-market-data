# ðŸ§  S&P 500 Data Engineering Pipeline

A personal project to practice **data engineering** concepts and tools â€” including **Python**, **Airflow**, **dbt**, **BigQuery**, and **Docker** â€” by building a fully automated pipeline that collects and transforms **S&P 500 minute-interval stock data**.

---

## ðŸš€ Project Overview

This project simulates a modern data engineering workflow:

1. **Data Extraction** â€“ Python web scraper collects S&P 500 minute-level data.
2. **Data Storage** â€“ Raw data is stored in **Google BigQuery** (using the free Sandbox tier).
3. **Transformation** â€“ **dbt** models clean, aggregate, and prepare the data for analysis.
4. **Orchestration** â€“ **Apache Airflow** automates daily pipeline runs.
5. **Version Control** â€“ Code managed via **Git** and **GitHub**.
6. **Containerization** â€“ **Docker** ensures reproducible environments.

---

## ðŸ§° Tech Stack

| Tool | Purpose |
|------|----------|
| **Python** | Data extraction & utilities |
| **BeautifulSoup / Requests** | Web scraping |
| **BigQuery (Sandbox)** | Data warehouse |
| **dbt** | SQL-based data transformations |
| **Airflow** | Workflow orchestration |
| **Docker** | Containerized setup |
| **Git & GitHub** | Version control and collaboration |

---

## ðŸ§© Project Structure

